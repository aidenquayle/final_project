{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bad-gRwHWwwI"
      },
      "outputs": [],
      "source": [
        "#Importing packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import geopandas as gpd\n",
        "import plotly.express as px\n",
        "import folium\n",
        "import matplotlib\n",
        "!pip install mapclassify\n",
        "import mapclassify\n",
        "!pip install geodatasets\n",
        "from geodatasets import get_path\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13gHqg3f_lut"
      },
      "outputs": [],
      "source": [
        "#Reading the Data into Python\n",
        "data = pd.read_csv('/content/ufo_data.csv', on_bad_lines = 'skip')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPfyQmcVEV_W"
      },
      "source": [
        "## Preliminary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJb60bnmEUTe"
      },
      "outputs": [],
      "source": [
        "#Preliminary Analysis\n",
        "\n",
        "#Getting the names of the columns\n",
        "column_names = data.columns\n",
        "print(\"Column Names:\", column_names)\n",
        "\n",
        "#making the comments column into a readable format\n",
        "data['comments'] = data['comments'].astype(str)\n",
        "\n",
        "# Get column data types\n",
        "column_types = data.dtypes\n",
        "print(\"Column Types:\\n\", column_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWb0VzO6EzIi"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_geo(data,\n",
        "                     lat='latitude',\n",
        "                     lon='longitude',\n",
        "                     hover_name='comments',  # Displays comments when you hover over a point,\n",
        "                     title='UFO Sightings Worldwide')\n",
        "\n",
        "fig.update_layout(showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcQAJ_1ODLxy"
      },
      "outputs": [],
      "source": [
        "#Creating a plot of the number of UFO sightings by year\n",
        "data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
        "\n",
        "# Drop rows where datetime could not be parsed\n",
        "data = data.dropna(subset=['datetime'])\n",
        "\n",
        "# Aggregate data by year to count sightings per year\n",
        "data['year'] = data['datetime'].dt.year\n",
        "ufo_sightings_by_year = data.groupby('year').size()\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ufo_sightings_by_year.index, ufo_sightings_by_year.values, marker='o')\n",
        "plt.title('UFO Sightings by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Sightings')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmXoOwlCuwOS"
      },
      "source": [
        "## Performing Sentiment Analysis using VADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrLfuiTWErhF"
      },
      "outputs": [],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W', ' ', str(text))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "data['processed_comments'] = data['comments'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get the compound score\n",
        "def get_sentiment_score(text):\n",
        "    score = sia.polarity_scores(text)\n",
        "    return score['compound']  # Returning the compound score\n",
        "\n",
        "# Apply the function to processed comments\n",
        "data['VADER_sentiment_score'] = data['processed_comments'].apply(get_sentiment_score)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chKuDvGlt89a"
      },
      "outputs": [],
      "source": [
        "# Clean and convert the latitude and longitude to numeric\n",
        "data['latitude'] = pd.to_numeric(data['latitude'], errors='coerce')\n",
        "data['longitude'] = pd.to_numeric(data['longitude'], errors='coerce')\n",
        "\n",
        "# Drop any rows with NaN values in latitude or longitude\n",
        "data.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
        "\n",
        "def sentiment_category(score):\n",
        "    if score >= 0:\n",
        "        return 'positive'\n",
        "    elif score < 0:\n",
        "        return 'negative'\n",
        "\n",
        "\n",
        "data['VADER_sentiment_category'] = data['VADER_sentiment_score'].apply(sentiment_category)\n",
        "\n",
        "# Create a scatter plot on a world map\n",
        "fig3 = px.scatter_geo(data,\n",
        "                     lat='latitude',\n",
        "                     lon='longitude',\n",
        "                     color='VADER_sentiment_category',  # This will use the sentiment_category column for color\n",
        "                     hover_name='comments',  # Displays comments when you hover over a point\n",
        "                     hover_data=['VADER_sentiment_score'],  # Also display the sentiment score on hover\n",
        "                     opacity = .5,\n",
        "                     projection='natural earth',\n",
        "                     color_discrete_map={'positive': 'green','negative': 'red'},  # Custom color mapping\n",
        "                     title='UFO Sightings Sentiment Worldwide')\n",
        "\n",
        "fig3.update_layout(showlegend=True)\n",
        "fig3.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOZRQjtK7zH"
      },
      "source": [
        "## Performing Sentiment Analysis using Sentiment 140 data to create an RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "nRCorPwUKJXx",
        "outputId": "baa75fec-9add-4132-81ff-1a42e97c06e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-da0ff7190e18>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#loading the sentiment 140 dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msentiment140_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Sentiment140.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m610683\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msentiment140_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tweet_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'username'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m     \"\"\"\n\u001b[1;32m   1337\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#loading the sentiment 140 dataset\n",
        "sentiment140_data = pd.read_csv('/content/Sentiment140.csv', encoding='ISO-8859-1', header=None, skiprows=[610683])\n",
        "sentiment140_data.columns = ['polarity', 'tweet_id', 'date', 'query', 'username', 'text']\n",
        "\n",
        "\n",
        "sentiment_mapping = {0: 0, 2: 1, 4: 2}\n",
        "sentiment140_data['polarity'] = sentiment140_data['polarity'].map(sentiment_mapping)\n",
        "\n",
        "#Preprocessing the data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(sentiment140_data['text'].values)\n",
        "sequences = tokenizer.texts_to_sequences(sentiment140_data['text'].values)\n",
        "maxlen = 100\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = sentiment140_data['polarity'].values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "\n",
        "# Build the RNN model\n",
        "embedding_dim = 32\n",
        "rnn_units = 64\n",
        "num_classes = 3\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=embedding_dim, input_length=maxlen),\n",
        "    LSTM(rnn_units, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 5\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc:.2f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RGqt1E7pZlAV"
      },
      "outputs": [],
      "source": [
        "#Now that a RNN Model has been created, we can apply it to our UFO dataset; need to do some tuning so it is more relevant to our dataset\n",
        "comments = data['comments'].astype(str)\n",
        "\n",
        "# Convert the comments to sequences and pad them\n",
        "ufo_sequences = tokenizer.texts_to_sequences(comments)\n",
        "ufo_padded_sequences = pad_sequences(ufo_sequences, maxlen=maxlen)\n",
        "\n",
        "# Predict sentiment using the trained model\n",
        "model_predictions = model.predict(ufo_padded_sequences)\n",
        "\n",
        "# Get the predicted labels\n",
        "predicted_labels = np.argmax(model_predictions, axis=1)\n",
        "\n",
        "\n",
        "# Map numeric labels to sentiment labels\n",
        "sentiment_labels = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "data['RNN_predicted_sentiment'] = [sentiment_labels[label] for label in predicted_labels]\n",
        "\n",
        "# Display the head of the dataset to check\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ttqJrUWaepPz"
      },
      "outputs": [],
      "source": [
        "fig5 = px.scatter_geo(data,\n",
        "                     lat='latitude',\n",
        "                     lon='longitude',\n",
        "                     color='RNN_predicted_sentiment',  # This will use the sentiment_category column for color\n",
        "                     hover_name='comments',  # Displays comments when you hover over a point\n",
        "                     color_discrete_map={'positive': 'green','negative': 'red'},  # Custom color mapping\n",
        "                     projection='natural earth',\n",
        "                     opacity = .5,\n",
        "                     title='UFO Sightings Sentiment Worldwide')\n",
        "\n",
        "fig5.update_layout(showlegend=True)\n",
        "fig5.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KPbYBdK2vaSM"
      },
      "outputs": [],
      "source": [
        "#Printing out the number of negative and positive sentiments\n",
        "print(f\"Number of sentiment entries for VADER:{data['VADER_sentiment_category'].value_counts()}\")\n",
        "print()\n",
        "\n",
        "print(f\"Number of sentiment entries for RNN:{data['RNN_predicted_sentiment'].value_counts()}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QFkyM7mpbbe2"
      },
      "outputs": [],
      "source": [
        "#Prints out the number of sentiments that are the same between VADER and the RNN prediction\n",
        "vader_rnn_matches = (data['VADER_sentiment_category'] == data['RNN_predicted_sentiment']).sum()\n",
        "\n",
        "print(f\"Number of matching sentiment entries: {vader_rnn_matches}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRvhB2hJcPt3"
      },
      "outputs": [],
      "source": [
        "# Graphing sentiment over time for the RNN\n",
        "data.set_index('datetime', inplace=True)\n",
        "\n",
        "# Resample the data annually and count sentiments\n",
        "rnn_sentiment_counts = data.resample('A').RNN_predicted_sentiment.value_counts().unstack()\n",
        "\n",
        "# Plot the data\n",
        "rnn_sentiment_counts.plot(kind='line', figsize=(10, 5))\n",
        "plt.title('Positive and Negative RNN Sentiment Encounters Per Year')\n",
        "plt.ylabel('Number of Encounters')\n",
        "plt.xlabel('Year')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#graphing sentiment over time for VADER\n",
        "vader_sentiment_counts = data.resample('A').VADER_sentiment_category.value_counts().unstack()\n",
        "\n",
        "# Plot the data\n",
        "vader_sentiment_counts.plot(kind='line', figsize=(10, 5))\n",
        "plt.title('Positive and Negative VADER Sentiment Encounters Per Year')\n",
        "plt.ylabel('Number of Encounters')\n",
        "plt.xlabel('Year')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}